 ~  $ cd spark-2.4.2-bin-hadoop2.7/python/
 ~/spark-2.4.2-bin-hadoop2.7/python  $ python
Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import pyspark
s>>> spark = pyspark.sql.SparkSession.appName('KMeansCluster').getOrCreate()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: type object 'SparkSession' has no attribute 'appName'
>>> spark = pyspark.sql.SparkSession.builder.appName('KMeansCluster').getOrCreate()
19/09/01 13:01:56 WARN Utils: Your hostname, gustavoarsenio-Inspiron-5458 resolves to a loopback address: 127.0.1.1; using 192.168.1.35 instead (on interface enp7s0)
19/09/01 13:01:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/jars/spark-unsafe_2.12-2.4.2.jar) to method java.nio.Bits.unaligned()
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
19/09/01 13:01:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
>>> dataset = spark.read.format('libsvm').load('sample_kmeans_data.txt')
19/09/01 13:03:30 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.
>>>                                                                                                                                                                                                                                                                                            dataset.show()
  File "<stdin>", line 1
    dataset.show()
    ^
IndentationError: unexpected indent
>>> dataset.show()
+-----+--------------------+
|label|            features|
+-----+--------------------+
|  0.0|           (3,[],[])|
|  1.0|(3,[0,1,2],[0.1,0...|
|  2.0|(3,[0,1,2],[0.2,0...|
|  3.0|(3,[0,1,2],[9.0,9...|
|  4.0|(3,[0,1,2],[9.1,9...|
|  5.0|(3,[0,1,2],[9.2,9...|
+-----+--------------------+

>>> final_data = dataset.select('features')
>>> final_data
DataFrame[features: vector]
>>> from pyspark.ml.clustering import KMeans
>>> kmeans = KMeans().setK(2).setSeed(1)
>>> model = kmeans.fit(final_data)
19/09/01 13:06:53 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
19/09/01 13:06:53 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
>>> wssse = model.computeCost(final_data)                                       
>>> wssse
0.11999999999994547
>>> final_data.show()
+--------------------+
|            features|
+--------------------+
|           (3,[],[])|
|(3,[0,1,2],[0.1,0...|
|(3,[0,1,2],[0.2,0...|
|(3,[0,1,2],[9.0,9...|
|(3,[0,1,2],[9.1,9...|
|(3,[0,1,2],[9.2,9...|
+--------------------+

>>> centers = model.clusterCenters()
>>> centers
[array([0.1, 0.1, 0.1]), array([9.1, 9.1, 9.1])]
>>> result = model.transform(final_data)
>>> resutl
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'resutl' is not defined
>>> result
DataFrame[features: vector, prediction: int]
>>> result.show()
+--------------------+----------+
|            features|prediction|
+--------------------+----------+
|           (3,[],[])|         0|
|(3,[0,1,2],[0.1,0...|         0|
|(3,[0,1,2],[0.2,0...|         0|
|(3,[0,1,2],[9.0,9...|         1|
|(3,[0,1,2],[9.1,9...|         1|
|(3,[0,1,2],[9.2,9...|         1|
+--------------------+----------+

>>> # Mais valores de J
... K*
  File "<stdin>", line 2
    K*
     ^
SyntaxError: invalid syntax
>>> kmeans = KMeans().setK(3).setSeed(1)
>>> model = kmeans.fit(final_data)
>>> wssse = model.computeCost(final_data)
>>> centers = model.clusterCenters()
>>> result = model.transform(final_data)
>>> wssse = model.computeCost(final_data)
>>> wsse
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'wsse' is not defined
>>> wssse
0.07499999999994544
>>> centers
[array([9.1, 9.1, 9.1]), array([0.05, 0.05, 0.05]), array([0.2, 0.2, 0.2])]
>>> result.show()
+--------------------+----------+
|            features|prediction|
+--------------------+----------+
|           (3,[],[])|         1|
|(3,[0,1,2],[0.1,0...|         1|
|(3,[0,1,2],[0.2,0...|         2|
|(3,[0,1,2],[9.0,9...|         0|
|(3,[0,1,2],[9.1,9...|         0|
|(3,[0,1,2],[9.2,9...|         0|
+--------------------+----------+

>>> 
[1]+  Stopped                 python3
 ~/spark-2.4.2-bin-hadoop2.7/python  $ 
