gustavoarsenio@gustavoarsenio-Inspiron-5458:~/spark-2.4.2-bin-hadoop2.7/python$ on/
gustavoarsenio@gustavoarsenio-Inspiron-5458:~/spark-2.4.2-bin-hadoop2.7/python$ python
Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import pyspark
>>> spark = pyspark.sql.SparkSession.builder.appName('kMeans_code').getOrCreate()
19/09/01 13:36:15 WARN Utils: Your hostname, gustavoarsenio-Inspiron-5458 resolves to a loopback address: 127.0.1.1; using 192.168.1.35 instead (on interface enp7s0)
19/09/01 13:36:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/jars/spark-unsafe_2.12-2.4.2.jar) to method java.nio.Bits.unaligned()
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
19/09/01 13:36:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
>>> 
>>> data_set= spark.read.csv('seeds_dataset.csv',header=True,inferSchema=True)
>>> data_set.printSchema()                                                      
root
 |-- area: double (nullable = true)
 |-- perimeter: double (nullable = true)
 |-- compactness: double (nullable = true)
 |-- length_of_kernel: double (nullable = true)
 |-- width_of_kernel: double (nullable = true)
 |-- asymmetry_coefficient: double (nullable = true)
 |-- length_of_groove: double (nullable = true)

>>> data_set.show()
+-----+---------+-----------+------------------+------------------+---------------------+------------------+
| area|perimeter|compactness|  length_of_kernel|   width_of_kernel|asymmetry_coefficient|  length_of_groove|
+-----+---------+-----------+------------------+------------------+---------------------+------------------+
|15.26|    14.84|      0.871|             5.763|             3.312|                2.221|              5.22|
|14.88|    14.57|     0.8811| 5.553999999999999|             3.333|                1.018|             4.956|
|14.29|    14.09|      0.905|             5.291|3.3369999999999997|                2.699|             4.825|
|13.84|    13.94|     0.8955|             5.324|3.3789999999999996|                2.259|             4.805|
|16.14|    14.99|     0.9034|5.6579999999999995|             3.562|                1.355|             5.175|
|14.38|    14.21|     0.8951|             5.386|             3.312|   2.4619999999999997|             4.956|
|14.69|    14.49|     0.8799|             5.563|             3.259|   3.5860000000000003| 5.218999999999999|
|14.11|     14.1|     0.8911|              5.42|             3.302|                  2.7|               5.0|
|16.63|    15.46|     0.8747|             6.053|             3.465|                 2.04| 5.877000000000001|
|16.44|    15.25|      0.888|5.8839999999999995|             3.505|                1.969|5.5329999999999995|
|15.26|    14.85|     0.8696|5.7139999999999995|             3.242|                4.543|             5.314|
|14.03|    14.16|     0.8796|             5.438|             3.201|   1.7169999999999999|             5.001|
|13.89|    14.02|      0.888|             5.439|             3.199|                3.986|             4.738|
|13.78|    14.06|     0.8759|             5.479|             3.156|                3.136|             4.872|
|13.74|    14.05|     0.8744|             5.482|             3.114|                2.932|             4.825|
|14.59|    14.28|     0.8993|             5.351|             3.333|                4.185| 4.781000000000001|
|13.99|    13.83|     0.9183|             5.119|             3.383|                5.234| 4.781000000000001|
|15.69|    14.75|     0.9058|             5.527|             3.514|                1.599|             5.046|
| 14.7|    14.21|     0.9153|             5.205|             3.466|                1.767|             4.649|
|12.72|    13.57|     0.8686|             5.226|             3.049|                4.102|             4.914|
+-----+---------+-----------+------------------+------------------+---------------------+------------------+
only showing top 20 rows

>>> from pyspark.ml.clustering import KMeans
>>> from pyspark.ml.feature import VectorAssembler
['area', 'perimeter', 'compactness', 'length_of_kernel', 'width_of_kernel', 'asymmetry_coefficient', 'length_of_groove']
>>> assembler = VectorAssembler(inputCols=data_set.columns,outputCol='features')
>>> final_data = assembler.transform(data_set)
>>> final_data.printSchema()
root
 |-- area: double (nullable = true)
 |-- perimeter: double (nullable = true)
 |-- compactness: double (nullable = true)
 |-- length_of_kernel: double (nullable = true)
 |-- width_of_kernel: double (nullable = true)
 |-- asymmetry_coefficient: double (nullable = true)
 |-- length_of_groove: double (nullable = true)
 |-- features: vector (nullable = true)

>>> from pyspark.ml.feature import StandardScaler
>>> scaler = StandardScaler(inputCol='features',outputCol='scaledFeatures')
>>> scaler_model = scaler.fit(final_data)
>>> final_data = scaler_model.transform(final_data)
>>> final_data.head(1)
[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22, features=DenseVector([15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22]), scaledFeatures=DenseVector([5.2445, 11.3633, 36.8608, 13.0072, 8.7685, 1.4772, 10.621]))]
>>> from pyspark.ml.clustering import KMeans
>>> kmeans = KMeans(featuresCol='scaledFeatures',k=3)
>>> model = kmeans.fit(final_data)
19/09/01 14:24:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
19/09/01 14:24:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
>>> model.computeCost(final_data)
428.60820118716356
>>> centers = model.clusterCenters()
>>> centers
[array([ 6.35645488, 12.40730852, 37.41990178, 13.93860446,  9.7892399 ,
        2.41585013, 12.29286107]), array([ 4.96198582, 10.97871333, 37.30930808, 12.44647267,  8.62880781,
        1.80061978, 10.41913733]), array([ 4.07497225, 10.14410142, 35.89816849, 11.80812742,  7.54416916,
        3.15410901, 10.38031464])]
>>> model.transform(final_data).select('prediction').show()
+----------+
|prediction|
+----------+
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         0|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         1|
|         2|
+----------+
only showing top 20 rows

