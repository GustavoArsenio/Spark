gustavoarsenio@gustavoarsenio-Inspiron-5458:~/spark-2.4.2-bin-hadoop2.7/python$ on/
gustavoarsenio@gustavoarsenio-Inspiron-5458:~/spark-2.4.2-bin-hadoop2.7/python$ pytho0n

Command 'pytho0n' not found, did you mean:

  command 'python0' from snap python0 (0.9.1)
  command 'python' from deb python3
  command 'python' from deb python
  command 'python' from deb python-minimal

See 'snap info <snapname>' for additional versions.

gustavoarsenio@gustavoarsenio-Inspiron-5458:~/spark-2.4.2-bin-hadoop2.7/python$ python
Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import pyspark
>>> import pyspark
>>> from pyspark.ml.clustering import KMeans
>>> spark = pyspark.sql.SparkSession.builder.appName('hack_test').getOrCreate()
19/09/01 14:36:02 WARN Utils: Your hostname, gustavoarsenio-Inspiron-5458 resolves to a loopback address: 127.0.1.1; using 192.168.1.35 instead (on interface enp7s0)
19/09/01 14:36:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/jars/spark-unsafe_2.12-2.4.2.jar) to method java.nio.Bits.unaligned()
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
19/09/01 14:36:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
>>> dataset = spark.read.csv('hack_data.csv',header=True,inferSchema=True)

>>> 
>>> dataset.show()
+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+
|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|
+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+
|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|
|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|
|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|
|                    2.0|           228.08|              1|             2.48|            8.0|             Bolivia|            70.8|
|                   20.0|            408.5|              0|             3.57|            8.0|                Iraq|           71.28|
|                    1.0|           390.69|              1|             2.79|            9.0|    Marshall Islands|           71.57|
|                   18.0|           342.97|              1|              5.1|            7.0|             Georgia|           72.32|
|                   22.0|           101.61|              1|             3.03|            7.0|         Timor-Leste|           72.03|
|                   15.0|           275.53|              1|             3.53|            8.0|Palestinian Terri...|           70.17|
|                   12.0|           424.83|              1|             2.53|            8.0|          Bangladesh|           69.99|
|                   15.0|           249.09|              1|             3.39|            9.0|Northern Mariana ...|           70.77|
|                   32.0|           242.48|              0|             4.24|            8.0|            Zimbabwe|           67.93|
|                   23.0|           514.54|              0|             3.18|            8.0|         Isle of Man|           68.56|
|                    9.0|           284.77|              0|             3.12|            9.0|Sao Tome and Prin...|           70.82|
|                   27.0|           779.25|              1|             2.37|            8.0|              Greece|           72.73|
|                   12.0|           307.31|              1|             3.22|            7.0|     Solomon Islands|           67.95|
|                   21.0|           355.94|              1|              2.0|            7.0|       Guinea-Bissau|            72.0|
|                   10.0|           372.65|              0|             3.33|            7.0|        Burkina Faso|           69.19|
|                   20.0|           347.23|              1|             2.33|            7.0|            Mongolia|           70.41|
|                   22.0|           456.57|              0|             1.52|            8.0|             Nigeria|           69.35|
+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+
only showing top 20 rows

>>> import pyspark.ml.feature import VectorAssembler
  File "<stdin>", line 1
    import pyspark.ml.feature import VectorAssembler
                                   ^
SyntaxError: invalid syntax
>>> from pyspark.ml.feature import VectorAssembler
>>> dataset.columns
['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used', 'Servers_Corrupted', 'Pages_Corrupted', 'Location', 'WPM_Typing_Speed']
>>> feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used', 'Servers_Corrupted', 'Pages_Corrupted', 'WPM_Typing_Speed']
>>> assembler = VectorAssembler(inputCols=feat_cols,output='features')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/python/pyspark/__init__.py", line 110, in wrapper
    return func(self, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'output'
>>> assembler = VectorAssembler(inputCols=feat_cols,outputCol='features')
>>> final_data = assembler.transform(dataset)
>>> final_data.printSchema
<bound method DataFrame.printSchema of DataFrame[Session_Connection_Time: double, Bytes Transferred: double, Kali_Trace_Used: int, Servers_Corrupted: double, Pages_Corrupted: double, Location: string, WPM_Typing_Speed: double, features: vector]>
>>> final_data.printSchema()
root
 |-- Session_Connection_Time: double (nullable = true)
 |-- Bytes Transferred: double (nullable = true)
 |-- Kali_Trace_Used: integer (nullable = true)
 |-- Servers_Corrupted: double (nullable = true)
 |-- Pages_Corrupted: double (nullable = true)
 |-- Location: string (nullable = true)
 |-- WPM_Typing_Speed: double (nullable = true)
 |-- features: vector (nullable = true)

>>> from pyspark.ml.feature import StandardScaler
>>> scaler = StandardScaler(inputCol='features',outputCol='scaledFeature')
>>> scaler_model = scaler.fit(final_data)
>>> cluster_final_data = scaler_model.transform(final_data)
>>> cluster_final_data.printSchema()
root
 |-- Session_Connection_Time: double (nullable = true)
 |-- Bytes Transferred: double (nullable = true)
 |-- Kali_Trace_Used: integer (nullable = true)
 |-- Servers_Corrupted: double (nullable = true)
 |-- Pages_Corrupted: double (nullable = true)
 |-- Location: string (nullable = true)
 |-- WPM_Typing_Speed: double (nullable = true)
 |-- features: vector (nullable = true)
 |-- scaledFeature: vector (nullable = true)

>>> kmeans2 = KMeans(featuresCol='scaledFeature',k=2)
>>> kmeans = KMeans(featuresCol='scaledFeature',k=3)
>>> del kmeans
>>> kmeans3 = KMeans(featuresCol='scaledFeature',k=3)
>>> model_km2 = kmeans2.fit(cluster_final_data)
19/09/01 14:47:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
19/09/01 14:47:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
>>> model_km3 = kmeans3.fit(cluster_final_data)                                
>>> model_km3.transform(cluster_final_data).select('predictions').show()
Exception ignored in: <object repr() failed>
Traceback (most recent call last):
  File "/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/python/pyspark/ml/wrapper.py", line 40, in __del__
    if SparkContext._active_spark_context and self._java_obj is not None:
AttributeError: 'VectorAssembler' object has no attribute '_java_obj'
Traceback (most recent call last):
  File "/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/home/gustavoarsenio/.local/lib/python3.6/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o199.select.
: org.apache.spark.sql.AnalysisException: cannot resolve '`predictions`' given input columns: [WPM_Typing_Speed, Bytes Transferred, scaledFeature, Pages_Corrupted, Servers_Corrupted, features, Location, Session_Connection_Time, Kali_Trace_Used, prediction];;
'Project ['predictions]
+- Project [Session_Connection_Time#10, Bytes Transferred#11, Kali_Trace_Used#12, Servers_Corrupted#13, Pages_Corrupted#14, Location#15, WPM_Typing_Speed#16, features#54, scaledFeature#72, UDF(scaledFeature#72) AS prediction#166]
   +- Project [Session_Connection_Time#10, Bytes Transferred#11, Kali_Trace_Used#12, Servers_Corrupted#13, Pages_Corrupted#14, Location#15, WPM_Typing_Speed#16, features#54, UDF(features#54) AS scaledFeature#72]
      +- Project [Session_Connection_Time#10, Bytes Transferred#11, Kali_Trace_Used#12, Servers_Corrupted#13, Pages_Corrupted#14, Location#15, WPM_Typing_Speed#16, UDF(named_struct(Session_Connection_Time, Session_Connection_Time#10, Bytes Transferred, Bytes Transferred#11, Kali_Trace_Used_double_VectorAssembler_f05092dafca5, cast(Kali_Trace_Used#12 as double), Servers_Corrupted, Servers_Corrupted#13, Pages_Corrupted, Pages_Corrupted#14, WPM_Typing_Speed, WPM_Typing_Speed#16)) AS features#54]
         +- Relation[Session_Connection_Time#10,Bytes Transferred#11,Kali_Trace_Used#12,Servers_Corrupted#13,Pages_Corrupted#14,Location#15,WPM_Typing_Speed#16] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:110)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:121)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:237)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:82)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1335)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:834)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py", line 1320, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/home/gustavoarsenio/.local/lib/python3.6/site-packages/py4j/java_gateway.py", line 1286, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/home/gustavoarsenio/spark-2.4.2-bin-hadoop2.7/python/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: "cannot resolve '`predictions`' given input columns: [WPM_Typing_Speed, Bytes Transferred, scaledFeature, Pages_Corrupted, Servers_Corrupted, features, Location, Session_Connection_Time, Kali_Trace_Used, prediction];;\n'Project ['predictions]\n+- Project [Session_Connection_Time#10, Bytes Transferred#11, Kali_Trace_Used#12, Servers_Corrupted#13, Pages_Corrupted#14, Location#15, WPM_Typing_Speed#16, features#54, scaledFeature#72, UDF(scaledFeature#72) AS prediction#166]\n   +- Project [Session_Connection_Time#10, Bytes Transferred#11, Kali_Trace_Used#12, Servers_Corrupted#13, Pages_Corrupted#14, Location#15, WPM_Typing_Speed#16, features#54, UDF(features#54) AS scaledFeature#72]\n      +- Project [Session_Connection_Time#10, Bytes Transferred#11, Kali_Trace_Used#12, Servers_Corrupted#13, Pages_Corrupted#14, Location#15, WPM_Typing_Speed#16, UDF(named_struct(Session_Connection_Time, Session_Connection_Time#10, Bytes Transferred, Bytes Transferred#11, Kali_Trace_Used_double_VectorAssembler_f05092dafca5, cast(Kali_Trace_Used#12 as double), Servers_Corrupted, Servers_Corrupted#13, Pages_Corrupted, Pages_Corrupted#14, WPM_Typing_Speed, WPM_Typing_Speed#16)) AS features#54]\n         +- Relation[Session_Connection_Time#10,Bytes Transferred#11,Kali_Trace_Used#12,Servers_Corrupted#13,Pages_Corrupted#14,Location#15,WPM_Typing_Speed#16] csv\n"
>>> model_km3.transform(cluster_final_data).printSchema()
root
 |-- Session_Connection_Time: double (nullable = true)
 |-- Bytes Transferred: double (nullable = true)
 |-- Kali_Trace_Used: integer (nullable = true)
 |-- Servers_Corrupted: double (nullable = true)
 |-- Pages_Corrupted: double (nullable = true)
 |-- Location: string (nullable = true)
 |-- WPM_Typing_Speed: double (nullable = true)
 |-- features: vector (nullable = true)
 |-- scaledFeature: vector (nullable = true)
 |-- prediction: integer (nullable = false)

>>> model_km3.transform(cluster_final_data).select('prediction').show()
+----------+
|prediction|
+----------+
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
|         0|
+----------+
only showing top 20 rows

>>> model_km3.transform(cluster_final_data).groupBy('prediction').count().show()
+----------+-----+                                                              
|prediction|count|
+----------+-----+
|         1|   79|
|         2|   88|
|         0|  167|
+----------+-----+

>>> model_km3.transform(cluster_final_data).groupBy('prediction').count().show()
+----------+-----+                                                              
|prediction|count|
+----------+-----+
|         1|   79|
|         2|   88|
|         0|  167|
+----------+-----+

>>> model_km2.transform(cluster_final_data).groupBy('prediction').count().show()
+----------+-----+                                                              
|prediction|count|
+----------+-----+
|         1|  167|
|         0|  167|
+----------+-----+

>>> # K2 está certo pois o valor das porcentagens descrito no problema nao bate
... 
>>> 
